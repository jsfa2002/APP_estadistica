# -*- coding: utf-8 -*-
"""ReDim - An谩lisis Multivariado Avanzado"""

import streamlit as st
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
from io import BytesIO

# Machine Learning
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from xgboost import XGBClassifier, XGBRegressor
from sklearn.metrics import (confusion_matrix, accuracy_score, classification_report, 
                           mean_squared_error, r2_score, silhouette_score)
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans, DBSCAN
from prince import MCA
from sklearn.pipeline import Pipeline

# Configuraci贸n de la p谩gina
st.set_page_config(page_title="ReDim - An谩lisis Multivariado", layout="wide", page_icon="")
st.title(' ReDim: An谩lisis Multivariado Avanzado')

# CSS personalizado mejorado
st.markdown("""
    <style>
    .stApp { background-color: #f9f9f9; }
    h1 { color: #2E86C1; text-align: center; font-weight: 700; }
    h2 { color: #3498DB; border-bottom: 2px solid #3498DB; padding-bottom: 5px; }
    h3 { color: #5DADE2; }
    .sidebar .sidebar-content { background-color: #EBF5FB; }
    .stButton>button { background-color: #2E86C1; color: white; }
    .stDownloadButton>button { background-color: #28B463; color: white; }
    .stAlert { border-left: 5px solid #2E86C1; }
    </style>
    """, unsafe_allow_html=True)

# Sidebar para subir el archivo
st.sidebar.header(" Configuraci贸n de Datos")
uploaded_file = st.sidebar.file_uploader("Sube tu archivo (CSV o Excel)", type=["csv", "xlsx"])

# Variables globales
if uploaded_file is not None:
    if uploaded_file.name.endswith('.csv'):
        df = pd.read_csv(uploaded_file)
    else:
        df = pd.read_excel(uploaded_file)
    
    st.sidebar.success("Archivo cargado exitosamente!")
    
    # Identificar tipos de columnas
    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
    cat_cols = df.select_dtypes(include=['object', 'category']).columns
    
    # Mostrar vista previa de datos
    st.subheader(" Vista previa de los datos")
    with st.expander("Ver datos completos"):
        st.dataframe(df)
    
    # Mostrar informaci贸n b谩sica del dataset
    st.subheader(" Metadatos del Dataset")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Total de Registros", df.shape[0])
        st.write("**Variables Num茅ricas:**", list(numeric_cols))
    with col2:
        st.metric("Total de Variables", df.shape[1])
        st.write("**Variables Categ贸ricas:**", list(cat_cols))
    with col3:
        st.metric("Valores Faltantes", df.isnull().sum().sum())
        st.write("**Memoria Usada:**", f"{df.memory_usage().sum() / (1024*1024):.2f} MB")

    # Men煤 de an谩lisis mejorado
    analysis_type = st.sidebar.selectbox(
        "Selecciona el tipo de an谩lisis",
        ["EDA", "Modelos Predictivos", "Reducci贸n de Dimensionalidad", "An谩lisis de Clusters"],
        index=0
    )

    # ====================== EDA MEJORADO ======================
    if analysis_type == "EDA":
        st.subheader(" An谩lisis Exploratorio de Datos (EDA) Avanzado")
        
        tab1, tab2, tab3, tab4 = st.tabs(["Resumen", "Distribuciones", "Correlaciones", "Datos Faltantes"])
        
        with tab1:
            st.write("### Estad铆sticas Descriptivas")
            st.dataframe(df.describe().T.style.background_gradient(cmap='Blues'))
            
            st.write("### Tipos de Datos")
            dtype_df = pd.DataFrame(df.dtypes.value_counts()).reset_index()
            dtype_df.columns = ['Tipo', 'Conteo']
            fig = px.pie(dtype_df, values='Conteo', names='Tipo', title='Distribuci贸n de Tipos de Datos')
            st.plotly_chart(fig, use_container_width=True)
        
        with tab2:
            if len(numeric_cols) > 0:
                selected_num = st.selectbox("Selecciona variable num茅rica", numeric_cols)
                
                col1, col2 = st.columns(2)
                with col1:
                    fig = px.histogram(df, x=selected_num, nbins=30, title=f'Distribuci贸n de {selected_num}')
                    st.plotly_chart(fig, use_container_width=True)
                with col2:
                    fig = px.box(df, y=selected_num, title=f'Boxplot de {selected_num}')
                    st.plotly_chart(fig, use_container_width=True)
                
                # An谩lisis de outliers usando IQR
                Q1 = df[selected_num].quantile(0.25)
                Q3 = df[selected_num].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                outliers = df[(df[selected_num] < lower_bound) | (df[selected_num] > upper_bound)]
                
                if not outliers.empty:
                    st.warning(f"锔 Se detectaron {len(outliers)} outliers en {selected_num}")
                    st.dataframe(outliers)
            
            if len(cat_cols) > 0:
                selected_cat = st.selectbox("Selecciona variable categ贸rica", cat_cols)
                fig = px.bar(df[selected_cat].value_counts().reset_index(), 
                            x='count', y=selected_cat, 
                            title=f'Distribuci贸n de {selected_cat}',
                            orientation='h')
                st.plotly_chart(fig, use_container_width=True)
        
        with tab3:
            if len(numeric_cols) > 1:
                st.write("### Matriz de Correlaci贸n Num茅rica")
                corr_matrix = df[numeric_cols].corr()
                fig = px.imshow(corr_matrix, 
                              text_auto=True, 
                              aspect="auto",
                              color_continuous_scale='RdBu',
                              range_color=[-1, 1],
                              title='Matriz de Correlaci贸n')
                st.plotly_chart(fig, use_container_width=True)
                
                # Correlaciones m谩s altas y bajas
                corr_series = corr_matrix.unstack().sort_values(ascending=False)
                high_corr = corr_series[corr_series < 1].head(5)
                low_corr = corr_series.tail(5)
                
                st.write("**Correlaciones m谩s altas:**")
                st.write(high_corr)
                
                st.write("**Correlaciones m谩s bajas:**")
                st.write(low_corr)
        
        with tab4:
            st.write("### An谩lisis de Valores Faltantes")
            null_data = df.isnull().sum().reset_index()
            null_data.columns = ['Variable', 'Conteo Nulos']
            null_data['Porcentaje'] = (null_data['Conteo Nulos'] / len(df)) * 100
            
            fig = px.bar(null_data.sort_values('Porcentaje', ascending=False), 
                        x='Porcentaje', y='Variable', 
                        title='Porcentaje de Valores Faltantes por Variable',
                        orientation='h')
            st.plotly_chart(fig, use_container_width=True)
            
            # Estrategias para manejar nulos
            if null_data['Conteo Nulos'].sum() > 0:
                st.warning("隆Advertencia: Hay valores faltantes en tus datos!")
                st.write("**Opciones para manejar nulos:**")
                st.markdown("""
                - **Eliminar filas:** `df.dropna()`
                - **Eliminar columnas:** `df.dropna(axis=1)`
                - **Imputar con media/mediana:** `df.fillna(df.mean())`
                - **Imputar con moda:** `df.fillna(df.mode().iloc[0])`
                - **Interpolaci贸n:** `df.interpolate()`
                """)

    # ====================== MODELOS PREDICTIVOS MEJORADOS ======================
    elif analysis_type == "Modelos Predictivos":
        st.subheader(" Modelado Predictivo Avanzado")
        
        tab1, tab2, tab3 = st.tabs(["Configuraci贸n", "Entrenamiento", "Evaluaci贸n"])
        
        with tab1:
            target_var = st.selectbox("Selecciona la variable objetivo (Y)", df.columns)
            predictor_vars = st.multiselect("Selecciona las variables predictoras (X)", 
                                          df.columns.drop(target_var),
                                          default=list(df.columns.drop(target_var))[:3])
            
            # Determinar tipo de problema
            if len(df[target_var].unique()) <= 5 and df[target_var].dtype in ['object', 'category']:
                problem_type = "Clasificaci贸n"
                st.success(" Se detect贸 un problema de Clasificaci贸n")
            else:
                problem_type = "Regresi贸n"
                st.success(" Se detect贸 un problema de Regresi贸n")
            
            # Selecci贸n de modelos seg煤n el tipo de problema
            if problem_type == "Clasificaci贸n":
                model_options = {
                    "Regresi贸n Log铆stica": LogisticRegression(max_iter=1000),
                    "An谩lisis Discriminante Lineal (LDA)": LinearDiscriminantAnalysis(),
                    "An谩lisis Discriminante Cuadr谩tico (QDA)": QuadraticDiscriminantAnalysis(),
                    "Random Forest": RandomForestClassifier(),
                    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss')
                }
            else:
                model_options = {
                    "Regresi贸n Lineal": LinearRegression(),
                    "Regresi贸n Ridge": Ridge(),
                    "Regresi贸n Lasso": Lasso(),
                    "Random Forest": RandomForestRegressor(),
                    "XGBoost": XGBRegressor()
                }
            
            selected_models = st.multiselect("Selecciona modelos a comparar",
                                           list(model_options.keys()),
                                           default=list(model_options.keys())[:2])
            
            # Opciones avanzadas
            with st.expander("锔 Opciones Avanzadas"):
                test_size = st.slider("Tama帽o del conjunto de prueba", 0.1, 0.5, 0.2, 0.05)
                random_state = st.number_input("Semilla aleatoria", 0, 1000, 42)
                cv_folds = st.number_input("N煤mero de folds para validaci贸n cruzada", 2, 10, 5)
                
                # Hiperpar谩metros para GridSearch
                if st.checkbox("Optimizar hiperpar谩metros (GridSearch)"):
                    st.write("Configura los rangos para GridSearch")
                    param_grid = {}
                    for model_name in selected_models:
                        if model_name == "Random Forest":
                            param_grid[model_name] = {
                                'n_estimators': st.multiselect("n_estimators para Random Forest", 
                                                              [50, 100, 200], [100]),
                                'max_depth': st.multiselect("max_depth para Random Forest", 
                                                          [None, 5, 10], [None])
                            }
                        elif model_name == "XGBoost":
                            param_grid[model_name] = {
                                'learning_rate': st.multiselect("learning_rate para XGBoost", 
                                                               [0.01, 0.1, 0.3], [0.1]),
                                'max_depth': st.multiselect("max_depth para XGBoost", 
                                                          [3, 6, 9], [6])
                            }
        
        with tab2:
            if st.button(" Entrenar Modelos"):
                st.session_state['model_results'] = {}
                
                X = df[predictor_vars]
                y = df[target_var]
                
                # Preprocesamiento
                if X.select_dtypes(include=['object']).any().any():
                    X = pd.get_dummies(X, drop_first=True)
                
                # Imputar valores faltantes
                imputer = SimpleImputer(strategy='mean' if problem_type == "Regresi贸n" else 'most_frequent')
                X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)
                
                # Escalar caracter铆sticas
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X)
                
                # Dividir datos
                X_train, X_test, y_train, y_test = train_test_split(
                    X_scaled, y, test_size=test_size, random_state=random_state)
                
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                for i, model_name in enumerate(selected_models):
                    status_text.text(f"Entrenando {model_name}...")
                    progress_bar.progress((i + 1) / len(selected_models))
                    
                    model = model_options[model_name]
                    
                    # Entrenar modelo (con o sin GridSearch)
                    if 'param_grid' in locals() and model_name in param_grid:
                        grid_search = GridSearchCV(model, param_grid[model_name], 
                                                cv=cv_folds, scoring='accuracy' if problem_type == "Clasificaci贸n" else 'r2')
                        grid_search.fit(X_train, y_train)
                        best_model = grid_search.best_estimator_
                        best_params = grid_search.best_params_
                    else:
                        best_model = model
                        best_model.fit(X_train, y_train)
                        best_params = "No optimizado"
                    
                    # Evaluar modelo
                    y_pred = best_model.predict(X_test)
                    
                    if problem_type == "Clasificaci贸n":
                        accuracy = accuracy_score(y_test, y_pred)
                        report = classification_report(y_test, y_pred, output_dict=True)
                        cm = confusion_matrix(y_test, y_pred)
                        
                        st.session_state['model_results'][model_name] = {
                            'model': best_model,
                            'type': 'classification',
                            'accuracy': accuracy,
                            'report': report,
                            'confusion_matrix': cm,
                            'best_params': best_params
                        }
                    else:
                        mse = mean_squared_error(y_test, y_pred)
                        r2 = r2_score(y_test, y_pred)
                        
                        st.session_state['model_results'][model_name] = {
                            'model': best_model,
                            'type': 'regression',
                            'mse': mse,
                            'r2': r2,
                            'best_params': best_params,
                            'y_test': y_test,
                            'y_pred': y_pred
                        }
                
                status_text.text("隆Entrenamiento completado!")
                progress_bar.empty()
                st.balloons()
        
        with tab3:
            if 'model_results' in st.session_state and st.session_state['model_results']:
                st.subheader(" Resultados de los Modelos")
                
                best_model_name = None
                best_score = -np.inf
                
                for model_name, results in st.session_state['model_results'].items():
                    with st.expander(f" {model_name}", expanded=True):
                        st.write(f"**Par谩metros 贸ptimos:** `{results['best_params']}`")
                        
                        if results['type'] == 'classification':
                            st.write(f"**Exactitud:** {results['accuracy']:.4f}")
                            
                            # Matriz de confusi贸n
                            fig, ax = plt.subplots()
                            sns.heatmap(results['confusion_matrix'], annot=True, fmt='d', cmap='Blues', ax=ax)
                            ax.set_xlabel('Predicho')
                            ax.set_ylabel('Real')
                            ax.set_title(f'Matriz de Confusi贸n - {model_name}')
                            st.pyplot(fig)
                            
                            # Reporte de clasificaci贸n
                            st.write("**Reporte de Clasificaci贸n:**")
                            report_df = pd.DataFrame(results['report']).transpose()
                            st.dataframe(report_df.style.background_gradient(cmap='Blues'))
                            
                            # Actualizar mejor modelo
                            if results['accuracy'] > best_score:
                                best_score = results['accuracy']
                                best_model_name = model_name
                        else:
                            st.write(f"**Error Cuadr谩tico Medio (MSE):** {results['mse']:.4f}")
                            st.write(f"**Coeficiente de Determinaci贸n (R虏):** {results['r2']:.4f}")
                            
                            # Gr谩fico de valores reales vs predichos
                            fig = px.scatter(x=results['y_test'], y=results['y_pred'], 
                                           labels={'x': 'Valores Reales', 'y': 'Valores Predichos'},
                                           title=f'Valores Reales vs Predichos - {model_name}')
                            fig.add_shape(type='line', x0=results['y_test'].min(), y0=results['y_test'].min(),
                                        x1=results['y_test'].max(), y1=results['y_test'].max(),
                                        line=dict(color='Red', dash='dash'))
                            st.plotly_chart(fig, use_container_width=True)
                            
                            # Actualizar mejor modelo
                            if results['r2'] > best_score:
                                best_score = results['r2']
                                best_model_name = model_name
                
                if best_model_name:
                    st.success(f" Mejor modelo: {best_model_name} con {'exactitud' if problem_type == 'Clasificaci贸n' else 'R虏'} de {best_score:.4f}")
                    
                    # Exportar modelo
                    import joblib
                    model_bytes = BytesIO()
                    joblib.dump(st.session_state['model_results'][best_model_name]['model'], model_bytes)
                    model_bytes.seek(0)
                    
                    st.download_button(
                        label="猬锔 Descargar mejor modelo",
                        data=model_bytes,
                        file_name=f"best_model_{best_model_name.replace(' ', '_')}.pkl",
                        mime="application/octet-stream"
                    )

    # ====================== REDUCCIN DE DIMENSIONALIDAD ======================
    elif analysis_type == "Reducci贸n de Dimensionalidad":
        st.subheader(" Reducci贸n de Dimensionalidad")
        
        dim_method = st.radio("Selecciona el m茅todo", 
                            ["PCA (An谩lisis de Componentes Principales)", 
                             "MCA (An谩lisis de Correspondencias M煤ltiples)"])
        
        if dim_method == "PCA (An谩lisis de Componentes Principales)":
            if len(numeric_cols) < 2:
                st.error("Se necesitan al menos 2 variables num茅ricas para ejecutar PCA")
            else:
                # Selecci贸n de variables
                selected_vars = st.multiselect("Selecciona variables para PCA", 
                                             numeric_cols, default=list(numeric_cols))
                
                # Opciones de PCA
                n_components = st.slider("N煤mero de componentes", 2, min(10, len(selected_vars)), 2)
                scale_data = st.checkbox("Estandarizar datos", value=True)
                
                if st.button("Ejecutar PCA"):
                    # Preprocesamiento
                    df_pca = df[selected_vars].copy()
                    imputer = SimpleImputer(strategy='mean')
                    df_filled = pd.DataFrame(imputer.fit_transform(df_pca), columns=selected_vars)
                    
                    # Escalar si es necesario
                    if scale_data:
                        scaler = StandardScaler()
                        df_scaled = scaler.fit_transform(df_filled)
                    else:
                        df_scaled = df_filled.values
                    
                    # Aplicar PCA
                    pca = PCA(n_components=n_components)
                    pca.fit(df_scaled)
                    components = pca.transform(df_scaled)
                    
                    # Resultados
                    st.write("### Varianza explicada")
                    var_exp = pca.explained_variance_ratio_
                    cum_var_exp = np.cumsum(var_exp)
                    
                    fig = px.bar(x=range(1, n_components+1), y=var_exp,
                                labels={'x': 'Componente', 'y': 'Varianza explicada'},
                                title='Varianza explicada por componente')
                    fig.add_scatter(x=range(1, n_components+1), y=cum_var_exp, 
                                   mode='lines+markers', name='Acumulada')
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # Gr谩fico de componentes
                    st.write("### Gr谩fico de Componentes Principales")
                    pc_df = pd.DataFrame(components[:, :2], columns=['PC1', 'PC2'])
                    
                    # A帽adir variables categ贸ricas para colorear si existen
                    if len(cat_cols) > 0:
                        color_var = st.selectbox("Variable para colorear puntos", cat_cols)
                        pc_df[color_var] = df[color_var].values
                        
                        fig = px.scatter(pc_df, x='PC1', y='PC2', color=color_var,
                                        title='PCA: Componente 1 vs Componente 2',
                                        hover_data={color_var: True})
                    else:
                        fig = px.scatter(pc_df, x='PC1', y='PC2', 
                                       title='PCA: Componente 1 vs Componente 2')
                    
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # Cargas factoriales
                    st.write("### Cargas Factoriales")
                    loadings = pd.DataFrame(
                        pca.components_.T,
                        columns=[f'PC{i+1}' for i in range(n_components)],
                        index=selected_vars
                    )
                    st.dataframe(loadings.style.background_gradient(cmap='RdBu', axis=None, vmin=-1, vmax=1))
                    
                    # Exportar resultados
                    csv = loadings.to_csv(index=True).encode('utf-8')
                    st.download_button(
                        "猬锔 Descargar cargas factoriales",
                        csv,
                        "pca_loadings.csv",
                        "text/csv"
                    )
        
        else:  # MCA
            if len(cat_cols) < 2:
                st.warning("Se necesitan al menos dos columnas categ贸ricas para ejecutar MCA")
            else:
                # Selecci贸n de variables
                selected_cats = st.multiselect("Selecciona variables categ贸ricas para MCA", 
                                             cat_cols, default=list(cat_cols)[:min(5, len(cat_cols))])
                
                n_components = st.slider("N煤mero de dimensiones", 2, min(5, len(selected_cats)), 2)
                
                if st.button("Ejecutar MCA"):
                    df_mca = df[selected_cats].copy().astype(str)
                    
                    # Aplicar MCA
                    mca = MCA(n_components=n_components, n_iter=10, random_state=42)
                    mca.fit(df_mca)
                    
                    # Coordenadas
                    mca_coords = mca.row_coordinates(df_mca)
                    mca_col_coords = mca.column_coordinates(df_mca)
                    
                    # Inercia
                    eigenvalues = mca.eigenvalues_
                    total_inertia = sum(eigenvalues)
                    explained_inertia = [eig / total_inertia for eig in eigenvalues]
                    
                    # Resultados
                    st.write("### Inercia explicada")
                    fig = px.bar(x=range(1, n_components+1), y=explained_inertia,
                                labels={'x': 'Dimensi贸n', 'y': 'Proporci贸n de inercia explicada'},
                                title='Inercia explicada por dimensi贸n')
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # Gr谩fico de categor铆as
                    st.write("### Gr谩fico de Categor铆as")
                    col_coords_df = mca_col_coords.reset_index()
                    col_coords_df['Variable'] = col_coords_df['index'].str.split('_').str[0]
                    
                    fig = px.scatter(col_coords_df, x=0, y=1, color='Variable',
                                   hover_name='index', 
                                   labels={'0': 'Dimensi贸n 1', '1': 'Dimensi贸n 2'},
                                   title='MCA: Representaci贸n de categor铆as')
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # Exportar coordenadas
                    csv = mca_col_coords.to_csv(index=True).encode('utf-8')
                    st.download_button(
                        "猬锔 Descargar coordenadas MCA",
                        csv,
                        "mca_coordinates.csv",
                        "text/csv"
                    )

    # ====================== ANLISIS DE CLUSTERS ======================
    elif analysis_type == "An谩lisis de Clusters":
        st.subheader(" An谩lisis de Clusters")
        
        cluster_method = st.selectbox("Selecciona el m茅todo de clustering",
                                    ["K-Means", "DBSCAN"])
        
        # Selecci贸n de variables
        selected_vars = st.multiselect("Selecciona variables para clustering", 
                                     numeric_cols, default=list(numeric_cols)[:3])
        
        # Preprocesamiento
        df_cluster = df[selected_vars].copy()
        imputer = SimpleImputer(strategy='mean')
        df_filled = pd.DataFrame(imputer.fit_transform(df_cluster), columns=selected_vars)
        scaler = StandardScaler()
        df_scaled = scaler.fit_transform(df_filled)
        
        if cluster_method == "K-Means":
            # M茅todo del codo para determinar k
            st.write("### M茅todo del Codo para Determinar K ptimo")
            distortions = []
            max_clusters = min(10, len(df_scaled)-1)
            K = range(1, max_clusters+1)
            
            for k in K:
                kmeans = KMeans(n_clusters=k, random_state=42)
                kmeans.fit(df_scaled)
                distortions.append(kmeans.inertia_)
            
            fig = px.line(x=K, y=distortions, 
                         labels={'x': 'N煤mero de clusters', 'y': 'Inercia'},
                         title='M茅todo del Codo')
            fig.update_traces(mode='lines+markers')
            st.plotly_chart(fig, use_container_width=True)
            
            # Seleccionar n煤mero de clusters
            n_clusters = st.slider("N煤mero de clusters", 2, max_clusters, 3)
            
            if st.button("Ejecutar K-Means"):
                kmeans = KMeans(n_clusters=n_clusters, random_state=42)
                clusters = kmeans.fit_predict(df_scaled)
                
                # A帽adir clusters al dataframe
                df_cluster['Cluster'] = clusters
                
                # Visualizaci贸n (usando PCA si hay m谩s de 2 variables)
                if len(selected_vars) > 2:
                    pca = PCA(n_components=2)
                    components = pca.fit_transform(df_scaled)
                    plot_df = pd.DataFrame(components, columns=['PC1', 'PC2'])
                    plot_df['Cluster'] = clusters
                    
                    fig = px.scatter(plot_df, x='PC1', y='PC2', color='Cluster',
                                   title='Visualizaci贸n de Clusters (PCA)')
                else:
                    plot_df = df_cluster.copy()
                    fig = px.scatter(plot_df, x=selected_vars[0], y=selected_vars[1], 
                                   color='Cluster', title='Visualizaci贸n de Clusters')
                
                st.plotly_chart(fig, use_container_width=True)
                
                # Estad铆sticas por cluster
                st.write("### Estad铆sticas por Cluster")
                cluster_stats = df_cluster.groupby('Cluster').mean()
                st.dataframe(cluster_stats.style.background_gradient(cmap='Blues'))
                
                # Silhouette score
                silhouette_avg = silhouette_score(df_scaled, clusters)
                st.write(f"**Silhouette Score:** {silhouette_avg:.3f}")
                st.caption("Valores cercanos a 1 indican clusters bien definidos.")
                
                # Exportar resultados
                csv = df_cluster.to_csv(index=False).encode('utf-8')
                st.download_button(
                    "猬锔 Descargar datos con clusters",
                    csv,
                    "data_with_clusters.csv",
                    "text/csv"
                )
        
        else:  # DBSCAN
            st.write("### Configuraci贸n de DBSCAN")
            eps = st.slider("EPS (Distancia m谩xima entre puntos)", 0.1, 2.0, 0.5, 0.1)
            min_samples = st.slider("M铆nimo de muestras por cluster", 1, 20, 5)
            
            if st.button("Ejecutar DBSCAN"):
                dbscan = DBSCAN(eps=eps, min_samples=min_samples)
                clusters = dbscan.fit_predict(df_scaled)
                
                # A帽adir clusters al dataframe
                df_cluster['Cluster'] = clusters
                n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)
                n_noise = list(clusters).count(-1)
                
                st.write(f"**N煤mero de clusters encontrados:** {n_clusters}")
                st.write(f"**Puntos considerados ruido:** {n_noise}")
                
                # Visualizaci贸n
                if len(selected_vars) > 2:
                    pca = PCA(n_components=2)
                    components = pca.fit_transform(df_scaled)
                    plot_df = pd.DataFrame(components, columns=['PC1', 'PC2'])
                    plot_df['Cluster'] = clusters
                    
                    fig = px.scatter(plot_df, x='PC1', y='PC2', color='Cluster',
                                   title='Visualizaci贸n de Clusters DBSCAN (PCA)')
                else:
                    plot_df = df_cluster.copy()
                    fig = px.scatter(plot_df, x=selected_vars[0], y=selected_vars[1], 
                                   color='Cluster', title='Visualizaci贸n de Clusters DBSCAN')
                
                st.plotly_chart(fig, use_container_width=True)
                
                # Exportar resultados
                csv = df_cluster.to_csv(index=False).encode('utf-8')
                st.download_button(
                    "猬锔 Descargar datos con clusters",
                    csv,
                    "data_with_clusters_dbscan.csv",
                    "text/csv"
                )

# Footer
st.sidebar.markdown("---")
st.sidebar.markdown("""
 **ReDim Team**   
""")

# Notas finales
with st.expander("癸 Acerca de esta aplicaci贸n"):
    st.markdown("""
    **ReDim - An谩lisis Multivariado Avanzado**  
    Versi贸n 2.0 | Mayo 2025  
    
    Esta aplicaci贸n permite realizar:
    - An谩lisis exploratorio de datos (EDA)
    - Modelado predictivo (clasificaci贸n y regresi贸n)
    - Reducci贸n de dimensionalidad (PCA, MCA)
    - An谩lisis de clusters (K-Means, DBSCAN)
    
    Desarrollado con Python, Streamlit y Scikit-learn.
    """)

